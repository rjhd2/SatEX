import numpy as np
import iris
import matplotlib.pyplot as plt
import iris.quickplot as qplt
import matplotlib.cm as mpl_cm
import iris.plot as iplt
import cartopy.crs as ccrs
import cartopy.feature as cfeature
import glob
from iris.util import unify_time_units
import datetime
import numpy.ma as ma
import sys
#import requests


def plot_daily_cycle(data):
	plt.close()
	fig=plt.figure()
	plt.plot(data)
	plt.grid()
	plt.title('CM SAF daily cycle for specific gridpoint')
	plt.xlabel('UTC hour of the day')
	plt.ylabel('LST in K')
	plt.savefig('/home/h01/vportge/CM_SAF/plots/CM_SAF/CM_SAF_time_series_LST'+day_yyyymmdd+'.png')
	
def plot_map(data):
	#new plot with filled contours
	plt.close()
	qplt.contourf(data)
	plt.gca().coastlines() #add coastlines
	plt.title('CM SAF map of LST of some hour')
	plt.savefig('/home/h01/vportge/CM_SAF/plots/CM_SAF/CM_SAF_map_LST'+day_yyyymmdd+'.png')


def plot_figure(data, gridlons, gridlats, title, outname):
	plt.close()
	fig = plt.figure()
	ax = fig.add_subplot(1, 1, 1,projection=ccrs.PlateCarree())
	lst_map=plt.contourf(gridlons, gridlats, data, transform=ccrs.PlateCarree())
	bar = plt.colorbar(lst_map, orientation='horizontal', extend='both')
	ax.coastlines()
	plt.title('CM SAF '+title)
	plt.savefig('/home/h01/vportge/CM_SAF/plots/CM_SAF/CM_SAF_map_LST_'+title[0:3]+'_'+outname+'.png')
	return


year_n_month=sys.argv[1] #has format: 1991/01/

files=glob.glob('/scratch/vportge/CM_SAF_data_metadata_changed/'+year_n_month+'*.nc') #filepaths of all hourly files of one month to be analysed.
#files=glob.glob('/scratch/vportge/CM_SAF_data_metadata_changed/*/*.nc') #filepaths of all hourly files of all days to be analysed.
files.sort()

get_coordinates_path=files[0:24]
get_coordinates_data=iris.load(get_coordinates_path, 'Land Surface Temperature (PMW)') 
get_coordinates_data=get_coordinates_data.concatenate_cube() #concatenate the hourly data into one cube with dimensions (time: 24; latitude: 856; longitude: 2171)

lons=get_coordinates_data.coord('longitude').points 
lats=get_coordinates_data.coord('latitude').points

###############################################################################
#Calculate the local time of each gridpoint. This depends on the longitude of #
#the gridpoint. +/- 1 degree longitude means: +/- 4 minutes to UTC time       #
#'offset' is the time period that has to be added/subtracted from the UTC time#
#and can be negative or positive. It is an array with dim(lons) as dimension. # 
############################################################################### 

offset=np.array([datetime.timedelta(i) for i in lons/360.]) #datetime(-1, 2281) means: go -1 day back and then plus 2281 seconds into the future, lons/360 gives the time in days which has to be added or subtracted from UTC time. offset is the same for all files.

time_data=get_coordinates_data.coord('time').points #those are the dates and times of each hourly file of one date. 
time_data_datetime=np.array([datetime.datetime.utcfromtimestamp(i) for i in time_data]) #make datetime object  out of timestamp 

#################################################################################
#Calculate the local times for each gridpoint for determining which hourly      #
#files lie within the time intervals 11 am to 3 pm and 4 am to 9 am where the   #
#maximum and minimum LST shall be analysed. The index/location of the hourly    #
#files inside the time array won't change when analysing different days and     #
#therefore the indices of the suitable hourly files must only be calculated once#
#The local times which are calculated here are only used to find the indices:   #
#local_times_to_find_indices							#
#################################################################################

local_times_to_find_indices=np.array([i+offset for i in time_data_datetime]) #compute local time for each longitude and each UTC hour
#timediff[0].__str__() #to see date and time in readable way


time15=time_data_datetime[15] #get the data and time of 3 pm, datetime object.
time11=time_data_datetime[11] #11 am
time4=time_data_datetime[4]   #4 am
time9=time_data_datetime[9]   #9 am

LST_warm_indices=[]
LST_cold_indices=[]
for i in range(len(lons)):
	if i%100==0:
		print(i)

	#compute for each local time the difference to 3pm, 11am, 4am, 9am to decide whether it lies within the two time 		intervals. 

	timediff15=(local_times_to_find_indices[:,i]-time15) 
	timediff11=(local_times_to_find_indices[:,i]-time11)

	timediff4=(local_times_to_find_indices[:,i]-time4)
	timediff9=(local_times_to_find_indices[:,i]-time9)
	
	#get the 'day' values of the timedifference arrays (datetime objects)
	#if the considered local time is earlier than the compared time: the 'day' value is -1
	#if the considered local time is later than the compared time: the 'day' value is 0

	diffdays15=np.array([i.days for i in timediff15])
	diffdays11=np.array([i.days for i in timediff11])

	diffdays4=np.array([i.days for i in timediff4])
	diffdays9=np.array([i.days for i in timediff9])

	
	#LST_warm_indices contains for each longitude the indices in time coordinate where the local time is between 11 am and 3 pm. (warm window) np.intersect1d(11am, 3pm)
	#LST_cold_indices contains for each longitude the indices in time coordinate where the local time is between 4 am and 9 am. (cold window)   

	LST_warm_indices.append(np.intersect1d(np.where(diffdays11>=0)[0],np.where(diffdays15<0)[0]))
	LST_cold_indices.append(np.intersect1d(np.where(diffdays4>=0)[0],np.where(diffdays9<0)[0]))

	

##############################################################################
#Start of the analysis of each day. 'files' contain the filepaths.           #
##############################################################################

#calculate number of days to be analysed from number of files in directory, as there are only the hourly files
number_of_days=int(len(files)/24)
print(number_of_days) 
for d in range(0, 3):#number_of_days):


	hourly_data_path=files[d*24:d*24+24] #one day consists of 24 hours. So take the first 24 files for the first day, the next 24 files for the second day and so on.
	hourly_data=iris.load(hourly_data_path, 'Land Surface Temperature (PMW)') 

	hourly_uncertainty=iris.load(hourly_data_path, 'Land Surface Temperature Uncertainty PMW') 

	day_cube=hourly_data.concatenate_cube() #concatenate the hourly data into one cube with dimensions (time: 24; latitude: 856; longitude: 2171)
	day_data=day_cube.data

	day_uncrty_cube=hourly_uncertainty.concatenate_cube() #concatenate the hourly data into one cube with dimensions (time: 24; latitude: 856; longitude: 2171)
	day_uncrty=day_uncrty_cube.data


	day_yyyymmdd=hourly_data_path[0][59:67]
	#day_yyyymmdd=hourly_data_path[0][60:68]
	print(day_yyyymmdd)

	#if the local times are whished then do:
	#times_day=day_cube.coord('time').points #those are the dates and times of each hourly file of the day 
	#times_day_datetime=np.array([datetime.datetime.utcfromtimestamp(i) for i in times_day]) #make datetime object  out of times
	#local_times=np.array([i+offset for i in times_day_datetime]) #compute local time for each longitude and each UTC hour

	####################################################################################
	#Compute the minimum/maximum LST: LST_cold_indices contains for each longitude       #
	#an array of the indices of the times that lie within the appropriate time interval#
	#Go through each longitude (for i in range(len(lons))) and read out the data inside#
	#the time interval (LST_cold_indices[i]) for each latitude (':' at second place).    #
	#Then find for each latitude the minimum of the LST inside the time interval. 	   #
	#np.amin(..., axis=0). During this process the masked_values from day_data change  #
	#their fill_value to 1.e20. Therefore: mask these values again. 		   #
	####################################################################################

	LSTwarm_max=np.array([np.amax(day_data[LST_warm_indices[i],:,i], axis=0) for i in range(len(lons))])	
	LSTcold_min=np.array([np.amin(day_data[LST_cold_indices[i],:,i], axis=0) for i in range(len(lons))])
	LSTcold_max=np.array([np.amax(day_data[LST_cold_indices[i],:,i], axis=0) for i in range(len(lons))]) #maximum value in cold window (in LST_cold_indices time interval), as LST_cold_indices is often affected by clouds. 

	LSTwarm_max=ma.masked_values(LSTwarm_max, 1.e20)
	LSTcold_min=ma.masked_values(LSTcold_min, 1.e20) #mask fill values
	LSTcold_max=ma.masked_values(LSTcold_max, 1.e20)

	LSTwarm_max=LSTwarm_max.T
	LSTcold_min=LSTcold_min.T #longitude and latitude coordinates were inverted compared to all other datasets.
	LSTcold_max=LSTcold_max.T

	#find uncertainty values:
	LST_warm_max_uncrty_index=np.array([np.argmax(day_data[LST_warm_indices[i],:,i], axis=0) for i in range(len(lons))]) #result is the index of the hour-index in LST_warm_indices, so result=1 would mean: first element of hour_indices_array LST_warm_indices. If LST_warm_indices=[12,13,14,15] then result=1 means: first value -> 13, take hour=13
	LST_cold_min_uncrty_index=np.array([np.argmin(day_data[LST_cold_indices[i],:,i], axis=0) for i in range(len(lons))])
	LST_cold_max_uncrty_index=np.array([np.argmax(day_data[LST_cold_indices[i],:,i], axis=0) for i in range(len(lons))])



	hourly_index_warm_max=np.array([LST_warm_indices[i][LST_warm_max_uncrty_index[i,:]] for i in range(len(lons))])
	hourly_index_warm_max=hourly_index_warm_max.T #array of hourly indices shape:(856,2171): which hour has maximum LST in warm window-> take this uncertainty value then

	hourly_index_cold_min=np.array([LST_cold_indices[i][LST_cold_min_uncrty_index[i,:]] for i in range(len(lons))])
	hourly_index_cold_min=hourly_index_cold_min.T #array of hourly indices shape:(856,2171): which hour has minimum LST in cold window-> take this uncertainty value then

	hourly_index_cold_max=np.array([LST_cold_indices[i][LST_cold_max_uncrty_index[i,:]] for i in range(len(lons))])
	hourly_index_cold_max=hourly_index_cold_max.T #array of hourly indices shape:(856,2171): which hour has maximum LST in cold window -> take this uncertainty value then



	uncrty_warm_max=np.zeros(hourly_index_warm_max.shape)
	uncrty_warm_max=ma.masked_values(uncrty_warm_max, 1.e20)

	uncrty_cold_min=np.zeros(hourly_index_cold_min.shape)
	uncrty_cold_min=ma.masked_values(uncrty_cold_min, 1.e20)

	uncrty_cold_max=np.zeros(hourly_index_cold_max.shape)
	uncrty_cold_max=ma.masked_values(uncrty_cold_max, 1.e20)

	for i in range(len(lats)):
		for j in range(len(lons)):
			uncrty_warm_max[i,j]=(day_uncrty[hourly_index_warm_max[i,j], i, j])
			uncrty_cold_min[i,j]=(day_uncrty[hourly_index_cold_min[i,j], i, j])
			uncrty_cold_max[i,j]=(day_uncrty[hourly_index_cold_max[i,j], i, j])


	#################################################################
	#Uncomment this if a plots are wished. Is 'Agg' environment set?#
	#################################################################
	
	#plot with filled contours
	#plot_map(day_cube[0,:,:])	
	
	#plot daily cycle:
	#plot=plot_daily_cycle(day_data[:,179,1592])

	#plot LST max and min
	#plot_figure(LSTwarm_max, lons, lats, 'max LST '+str(day_yyyymmdd),  str(day_yyyymmdd))
	#plot_figure(LSTcold_min, lons, lats, 'min LST '+str(day_yyyymmdd),  str(day_yyyymmdd))


	####################################################################
	#Create the netCDF file for LST max and LST min and uncertainties  #   
	#For this: Create Cubes out of the np.arrays with metadata and then#
	#save the cubes to netCDF files using the iris.save method. 	   #
	####################################################################

	timedim=hourly_data[0].coord('time')
	latdim=hourly_data[0].coord('latitude') 
	londim=hourly_data[0].coord('longitude')
	attri={"creator_name": "Veronika Portge", "creator_email": "veronika.portge@metoffice.gov.uk", "summary": "This file contains time-space aggregated Thematic Climate Data Records (TCDR) produced by geosatclim within the Satellite Application Facility on Climate Monitoring (CM SAF). It was processed to determine the maximum land surface temperature in the warm window from 11 am - 3 pm local time and the maximum and minimum land surface temperature in the cold window from 4 am - 9 am local time. "}


	#reshape arrays so that they've got a time dimension
	LSTwarm_max=np.reshape(LSTwarm_max, (1,LSTwarm_max.shape[0], LSTwarm_max.shape[1]))
	LSTcold_min=np.reshape(LSTcold_min, (1,LSTcold_min.shape[0], LSTcold_min.shape[1]))
	LSTcold_max=np.reshape(LSTcold_max, (1,LSTcold_max.shape[0], LSTcold_max.shape[1]))

	uncrty_warm_max=np.reshape(uncrty_warm_max, (1,uncrty_warm_max.shape[0], uncrty_warm_max.shape[1]))
	uncrty_cold_min=np.reshape(uncrty_cold_min, (1,uncrty_cold_min.shape[0], uncrty_cold_min.shape[1]))
	uncrty_cold_max=np.reshape(uncrty_cold_max, (1,uncrty_cold_max.shape[0], uncrty_cold_max.shape[1]))

	#now: create Cubes out of the arrays
	LSTwarm_max_cube=iris.cube.Cube(LSTwarm_max, long_name="Maximum Land Surface Temperature in Warm Window (PMW)", units="K", attributes=attri, dim_coords_and_dims=[(timedim, 0), (latdim, 1), (londim, 2)])
	LSTcold_min_cube=iris.cube.Cube(LSTcold_min, long_name="Minimum Land Surface Temperature in Cold Window (PMW)", units="K", attributes=attri, dim_coords_and_dims=[(timedim, 0), (latdim, 1), (londim, 2)])
	LSTcold_max_cube=iris.cube.Cube(LSTcold_max, long_name="Maximum Land Surface Temperature in Cold Window (PMW)", units="K", attributes=attri, dim_coords_and_dims=[(timedim, 0), (latdim, 1), (londim, 2)])

	uncrty_warm_max_cube=iris.cube.Cube(uncrty_warm_max, long_name="Uncertainty of Maximum Land Surface Temperature in Warm Window (PMW)", units="K", attributes=attri, dim_coords_and_dims=[(timedim, 0), (latdim, 1), (londim, 2)])
	uncrty_cold_min_cube=iris.cube.Cube(uncrty_cold_min, long_name="Uncertainty of Minimum Land Surface Temperature in Cold Window (PMW)", units="K", attributes=attri, dim_coords_and_dims=[(timedim, 0), (latdim, 1), (londim, 2)])
	uncrty_cold_max_cube=iris.cube.Cube(uncrty_cold_max, long_name="Uncertainty of Maximum Land Surface Temperature in Cold Window (PMW)", units="K", attributes=attri, dim_coords_and_dims=[(timedim, 0), (latdim, 1), (londim, 2)])

	#make CubeList out of different cubes
	cubelist=iris.cube.CubeList([LSTwarm_max_cube, LSTcold_min_cube, LSTcold_max_cube, uncrty_warm_max_cube, uncrty_cold_min_cube, uncrty_cold_max_cube])

	#save cubes
	iris.save(cubelist, "/scratch/vportge/CM_SAF_LST_MIN_MAX/"+str(day_yyyymmdd[0:4])+"/"+str(day_yyyymmdd[4:6])+"/LST_max_and_min_"+day_yyyymmdd+".nc")




'''
lat = 48.871236 ## your latitude
lon = 2.77928 ## your longitude

url = "http://api.geonames.org/timezoneJSON?formatted=true&lat={}&lng={}&username=demo".format(lat,lon)

r = requests.get(url) ## Make a request
return r.json()['timezoneId'] 

from datetime import datetime
from dateutil import tz

from_zone = tz.gettz('UTC')
to_zone = tz.gettz('America/New_York')

# utc = datetime.utcnow()
utc = datetime.strptime('2011-01-21 02:37:21', '%Y-%m-%d %H:%M:%S')

# Tell the datetime object that it's in UTC time zone since 
# datetime objects are 'naive' by default
utc = utc.replace(tzinfo=from_zone)

# Convert time zone
central = utc.astimezone(to_zone)
get_timezones_data_path=files[0:24]
get_timezones_data=iris.load(get_timezones_data_path, 'Land Surface Temperature (PMW)') 
get_timezones_data=get_timezones_data.concatenate_cube() #concatenate the hourly data into one cube with dimensions (time: 24; latitude: 856; longitude: 2171)

lons=get_timezones_data.coord('longitude').points
lats=get_timezones_data.coord('latitude').points

#timezones_array=np.chararray((lats.shape[0], lons.shape[0]))
timezones_array=[]

for i in range(500,len(lats)):
	for j in range(1000,len(lons)):
		lat=lats[i]
		lon=lons[j]
		url = "http://api.geonames.org/timezoneJSON?formatted=true&lat={}&lng={}&username=VPORTGE".format(lat,lon)
		r = requests.get(url) ## Make a request
		
		print(r.json()['timezoneId'])
		#timezones_array[i,j]=r.json()['timezoneId']
		timezones_array.append(r.json()['timezoneId'])
		#except:
			#continue


'''

#day_array=np.zeros((24, 856, 2171))
#for i in range(0,24):
	#day_array[i,:,:]=hourly_data[i].data
#np.ma.masked_where(day_array==-32767. , day_array)
#day=str('{:02}'.format(d))
#print(day)
#filepath='/net/home/h01/vportge/CM_SAF/test_data/ORD28974_europe/ORD28974/LTPin201511'+day+'*.nc'
#print(filepath)
